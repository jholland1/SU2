#!/bin/bash
#SBATCH --job-name=RAE2822_C6_NN_fine_trans
#SBATCH -N 1
#SBATCH --ntasks-per-node 20
# SBATCH --share
# SBATCH --mem-per-cpu=6553
# SBATCH --mem_bind=verbose,local
# SBATCH --tmp=64000
# SBATCH --cpu-bind=rank
#SBATCH --time=144:00:00
#SBATCH --account=hamiltonian
#SBATCH --mail-type=FAIL
#SBATCH --mail-type=BEGIN
#SBATCH --mail-type=END
#SBATCH --mail-user jholland@umd.edu
#############################################################
# change the below input variable to your own filename
#############################################################
~/.profile
#module load openmpi/intel/2015.0.3.032/1.8.6
#module load lapack
#echo "Hello Jon"
echo "PATH is:"
echo $PATH
echo "Python Path is:"
echo $PYTHONPATH
echo "Modules are:"
module list
echo "Python Version is:"
python -V
echo "HOSTNAME IS"
echo $HOSTNAME
ulimit -s unlimited
#export PBS_NODEFILE="/usr/local/slurm/bin/generate_pbs_nodefile"
echo "SLURM_JOB_NODELIST is: $SLURM_JOB_NODELIST"
#/usr/local/slurm/bin/generate_pbs_nodefile > NODEFILE
#rm -rf NODEFILE
#cp $SLURM_JOB_NODELIST .
#./build_rankfile.sh $SLURM_JOB_NODELIST > $TMPDIR/rankfile.txt
#cp $TMPDIR/rankfile.txt .
#export SU2_MPI_COMMAND="mpirun --bind-to core --map-by ppr:20:node --rank-by core -np %i %s"
#export SU2_MPI_COMMAND="mpirun -np %i %s" 
export SU2_MPI_COMMAND="mpirun --bind-to core --report-bindings -np %i %s"

/lustre/jholland/SU2/bin/shape_optimization.py -n $SLURM_NTASKS -g DISCRETE_ADJOINT -f turb_SA_FIML_RAE2822_INVERSE_LIFT.cfg -o BFGSG
